{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revision History: Changed the way Punctuations are handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.0.8)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.27.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (0.33.6)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (3.11.3)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (0.1.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorflow==2.1) (1.18.1)\n",
      "Requirement already satisfied: h5py in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (41.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.11.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2019.9.11)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n",
      "Requirement already satisfied: nltk in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: tika in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (1.23.1)\n",
      "Requirement already satisfied: requests in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tika) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from tika) (41.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests->tika) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests->tika) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests->tika) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from requests->tika) (2019.9.11)\n",
      "Requirement already satisfied: textblob in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "/usr/bin/sh: pip3: command not found\n",
      "Requirement already satisfied: scikit-learn in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ravikrishnak/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.1\n",
    "!pip install nltk\n",
    "!pip install tika\n",
    "!pip install textblob\n",
    "!pip3 install --upgrade numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle Paths\n",
    "import os\n",
    "\n",
    "# To remove Hyperlinks and Dates\n",
    "import re\n",
    "\n",
    "# To remove Puncutations\n",
    "import string\n",
    "\n",
    "'''\n",
    "# For Tokenization and Accessing Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "'''\n",
    "\n",
    "# This helps to remove the unnecessary words from our Text Data\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# To Parse the Input Data Files\n",
    "from tika import parser\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# In order to use the Libraries of Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# For Preprocessing the Text => To Tokenize the Text\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# If the Two Articles are of different length, pad_sequences will make the length equal\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Package for performing Numerical Operations\n",
    "import numpy as np\n",
    "\n",
    "# MatplotLib for Plotting Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To shuffle the Data\n",
    "from random import shuffle\n",
    "\n",
    "# To Partition the Data into Train Data and Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To add Regularizer in order to reduce Overfitting\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the Path of our Data\n",
    "Path_Of_Data = '/home/ravikrishnak/Desktop/REsume classification/Test'\n",
    "\n",
    "Labels_List = ['Python', 'Mechanical', 'SAP', 'Testing', 'Networking', 'Civil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Process_Data_And_Create_BOW(folder_path):\n",
    "  #creating empty lists in order to Create Resume Text and the respective Label\n",
    "  Resumes_List = []\n",
    "  Input_Files = []\n",
    "  for EachLabel in Labels_List:\n",
    "    for root, dirs, files in os.walk(os.path.join(folder_path, EachLabel),topdown=False):\n",
    "      if len(files)>= 0:\n",
    "          for file in files:\n",
    "            i = 0\n",
    "            if file.endswith('.pdf'):\n",
    "              #Access individual file\n",
    "              Full_Resume_Path = os.path.join(root, file)\n",
    "              # Parse the Data inside the file\n",
    "              file_data = parser.from_file(Full_Resume_Path)\n",
    "              # Extract the Content of the File\n",
    "              Resume_Text = file_data['content']\n",
    "\n",
    "              # Below Code removes the Hyperlinks in the Resume, like LinkedIn Profile, Certifications, etc..\n",
    "              HyperLink_Regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "              Text_Without_HL = re.sub(HyperLink_Regex, ' ', Resume_Text, flags=re.MULTILINE)\n",
    "\n",
    "              # Below Code removes the Date from the Resume\n",
    "              Date_regEx = r'(?:\\d{1,2}[-/th|st|nd|rd\\s]*)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?[a-z\\s,.]*(?:\\d{1,2}[-/th|st|nd|rd)\\s,]*)+(?:\\d{2,4})+'\n",
    "              CleanedText = re.sub(Date_regEx,' ',Text_Without_HL)\n",
    "\n",
    "              List_Of_All_Punctuations = list(string.punctuation)\n",
    "              Important_Punctuations = ['#', '.', '+' , '-'] #Add more, if any other Punctuation is observed as Important\n",
    "\n",
    "              NewLineChar = '\\n'\n",
    "\n",
    "              # Below Set Comprises all the Punctuations, which can be Removed from the Text of Resume\n",
    "              Total_Punct = len(List_Of_All_Punctuations)\n",
    "\n",
    "              for EachImpPunct in Important_Punctuations:\n",
    "                  for CountOfPunct in range(Total_Punct):\n",
    "                      if CountOfPunct == Total_Punct:\n",
    "                          break\n",
    "                      elif EachImpPunct == List_Of_All_Punctuations[CountOfPunct]:\n",
    "                          del List_Of_All_Punctuations[CountOfPunct]\n",
    "                          Total_Punct = Total_Punct - 1\n",
    "\n",
    "              List_Of_All_Punctuations.append(NewLineChar)\n",
    "\n",
    "              for EachPunct in List_Of_All_Punctuations:\n",
    "                  CleanedText = CleanedText.replace(EachPunct, \" \")\n",
    "\n",
    "              # Below Code converts all the Words in the Resume to Lowercase ======> Check if it has to come after Tokenization if Splitting Code is delet instead of integed\n",
    "              #Final_Cleaned_Resume_Text = Text_Without_Punct.lower()\n",
    "              Final_Cleaned_Resume_Text = CleanedText.lower()\n",
    "\n",
    "              #Code to remove Stopwords from each Resume\n",
    "              for word in STOPWORDS:            \n",
    "                  stop_token = word\n",
    "                  Resume_Text = Final_Cleaned_Resume_Text.replace(stop_token, ' ')            \n",
    "              Resumes_List.append(Resume_Text)\n",
    "              Input_Files.append(EachLabel + '_' + file)\n",
    "  return Input_Files, Resumes_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function and passing the path\n",
    "Input_Files, Resumes_List = Pre_Process_Data_And_Create_BOW(Path_Of_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_Files = ['Python_9.pdf', 'Python_13.pdf', 'Python_6.pdf', 'Python_5.pdf', 'Python_3.pdf', 'Python_8.pdf', 'Python_12.pdf', 'Python_2.pdf', 'Python_10.pdf', 'Python_11.pdf', 'Python_1.pdf', 'Python_14.pdf', 'Python_7.pdf', 'Python_4.pdf', 'Mechanical_9.pdf', 'Mechanical_13.pdf', 'Mechanical_6.pdf', 'Mechanical_5.pdf', 'Mechanical_3.pdf', 'Mechanical_8.pdf', 'Mechanical_12.pdf', 'Mechanical_2.pdf', 'Mechanical_10.pdf', 'Mechanical_11.pdf', 'Mechanical_1.pdf', 'Mechanical_14.pdf', 'Mechanical_7.pdf', 'Mechanical_4.pdf', 'SAP_6(1).pdf', 'SAP_6.pdf', 'SAP_5.pdf', 'SAP_3.pdf', 'SAP_12.pdf', 'SAP_2.pdf', 'SAP_17.pdf', 'SAP_10.pdf', 'SAP_20.pdf', 'SAP_19.pdf', 'SAP_16.pdf', 'SAP_1.pdf', 'SAP_18.pdf', 'SAP_16(1).pdf', 'Testing_9.pdf', 'Testing_13.pdf', 'Testing_6.pdf', 'Testing_5.pdf', 'Testing_3.pdf', 'Testing_8.pdf', 'Testing_12.pdf', 'Testing_2.pdf', 'Testing_10.pdf', 'Testing_11.pdf', 'Testing_1.pdf', 'Testing_14.pdf', 'Testing_7.pdf', 'Testing_4.pdf', 'Networking_9.pdf', 'Networking_6.pdf', 'Networking_5.pdf', 'Networking_3.pdf', 'Networking_8.pdf', 'Networking_12.pdf', 'Networking_15.pdf', 'Networking_2.pdf', 'Networking_10.pdf', 'Networking_11.pdf', 'Networking_1.pdf', 'Networking_14.pdf', 'Networking_7.pdf', 'Networking_4.pdf', 'Civil_9.pdf', 'Civil_13.pdf', 'Civil_6.pdf', 'Civil_5.pdf', 'Civil_3.pdf', 'Civil_8.pdf', 'Civil_12.pdf', 'Civil_2.pdf', 'Civil_10.pdf', 'Civil_11.pdf', 'Civil_1.pdf', 'Civil_14.pdf', 'Civil_7.pdf', 'Civil_4.pdf']\n"
     ]
    }
   ],
   "source": [
    "print('Input_Files = {}'.format(Input_Files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "# We want the Output of the Embedding Layer to be 64\n",
    "embedding_dim = 64\n",
    "#max_length = 200\n",
    "max_length = 800\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(Resumes_List)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the Word Tokens into Integer equivalents, before passing it to keras embedding layer\n",
    "Predict_Sequences = tokenizer.texts_to_sequences(Resumes_List)\n",
    "\n",
    "Predict_Padded = pad_sequences(Predict_Sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "# Contains an assets folder, saved_model.pb, and variables folder.\n",
    "!ls Resume_Classification_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 128)         66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 431,814\n",
      "Trainable params: 431,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "New_Model = tf.keras.models.load_model(\"Resume_Classification_Model/1\")\n",
    "\n",
    "New_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Python_9.pdf is Civil\n",
      "Prediction for Python_13.pdf is Networking\n",
      "Prediction for Python_6.pdf is Networking\n",
      "Prediction for Python_5.pdf is Networking\n",
      "Prediction for Python_3.pdf is Networking\n",
      "Prediction for Python_8.pdf is Networking\n",
      "Prediction for Python_12.pdf is Networking\n",
      "Prediction for Python_2.pdf is Networking\n",
      "Prediction for Python_10.pdf is Networking\n",
      "Prediction for Python_11.pdf is Networking\n",
      "Prediction for Python_1.pdf is Networking\n",
      "Prediction for Python_14.pdf is Networking\n",
      "Prediction for Python_7.pdf is Networking\n",
      "Prediction for Python_4.pdf is Networking\n",
      "Prediction for Mechanical_9.pdf is Networking\n",
      "Prediction for Mechanical_13.pdf is Networking\n",
      "Prediction for Mechanical_6.pdf is Networking\n",
      "Prediction for Mechanical_5.pdf is Networking\n",
      "Prediction for Mechanical_3.pdf is Networking\n",
      "Prediction for Mechanical_8.pdf is Networking\n",
      "Prediction for Mechanical_12.pdf is Networking\n",
      "Prediction for Mechanical_2.pdf is Networking\n",
      "Prediction for Mechanical_10.pdf is Networking\n",
      "Prediction for Mechanical_11.pdf is Networking\n",
      "Prediction for Mechanical_1.pdf is Networking\n",
      "Prediction for Mechanical_14.pdf is Networking\n",
      "Prediction for Mechanical_7.pdf is Networking\n",
      "Prediction for Mechanical_4.pdf is Networking\n",
      "Prediction for SAP_6(1).pdf is Networking\n",
      "Prediction for SAP_6.pdf is Networking\n",
      "Prediction for SAP_5.pdf is Networking\n",
      "Prediction for SAP_3.pdf is Networking\n",
      "Prediction for SAP_12.pdf is Networking\n",
      "Prediction for SAP_2.pdf is Networking\n",
      "Prediction for SAP_17.pdf is Networking\n",
      "Prediction for SAP_10.pdf is Networking\n",
      "Prediction for SAP_20.pdf is Networking\n",
      "Prediction for SAP_19.pdf is Networking\n",
      "Prediction for SAP_16.pdf is Networking\n",
      "Prediction for SAP_1.pdf is Networking\n",
      "Prediction for SAP_18.pdf is Networking\n",
      "Prediction for SAP_16(1).pdf is Networking\n",
      "Prediction for Testing_9.pdf is Networking\n",
      "Prediction for Testing_13.pdf is Networking\n",
      "Prediction for Testing_6.pdf is Networking\n",
      "Prediction for Testing_5.pdf is Networking\n",
      "Prediction for Testing_3.pdf is Networking\n",
      "Prediction for Testing_8.pdf is Networking\n",
      "Prediction for Testing_12.pdf is Networking\n",
      "Prediction for Testing_2.pdf is Networking\n",
      "Prediction for Testing_10.pdf is Networking\n",
      "Prediction for Testing_11.pdf is Networking\n",
      "Prediction for Testing_1.pdf is Networking\n",
      "Prediction for Testing_14.pdf is Networking\n",
      "Prediction for Testing_7.pdf is Networking\n",
      "Prediction for Testing_4.pdf is Networking\n",
      "Prediction for Networking_9.pdf is Networking\n",
      "Prediction for Networking_6.pdf is Civil\n",
      "Prediction for Networking_5.pdf is Mechanical\n",
      "Prediction for Networking_3.pdf is Networking\n",
      "Prediction for Networking_8.pdf is Networking\n",
      "Prediction for Networking_12.pdf is Networking\n",
      "Prediction for Networking_15.pdf is Networking\n",
      "Prediction for Networking_2.pdf is Networking\n",
      "Prediction for Networking_10.pdf is Networking\n",
      "Prediction for Networking_11.pdf is Networking\n",
      "Prediction for Networking_1.pdf is Networking\n",
      "Prediction for Networking_14.pdf is Mechanical\n",
      "Prediction for Networking_7.pdf is Networking\n",
      "Prediction for Networking_4.pdf is Mechanical\n",
      "Prediction for Civil_9.pdf is Networking\n",
      "Prediction for Civil_13.pdf is Networking\n",
      "Prediction for Civil_6.pdf is Networking\n",
      "Prediction for Civil_5.pdf is Networking\n",
      "Prediction for Civil_3.pdf is Networking\n",
      "Prediction for Civil_8.pdf is Networking\n",
      "Prediction for Civil_12.pdf is Networking\n",
      "Prediction for Civil_2.pdf is Networking\n",
      "Prediction for Civil_10.pdf is Networking\n",
      "Prediction for Civil_11.pdf is Networking\n",
      "Prediction for Civil_1.pdf is Networking\n",
      "Prediction for Civil_14.pdf is Networking\n",
      "Prediction for Civil_7.pdf is Networking\n",
      "Prediction for Civil_4.pdf is Networking\n"
     ]
    }
   ],
   "source": [
    "Prediction = New_Model.predict(Predict_Padded)\n",
    "\n",
    "Prediction_Classes = tf.argmax(Prediction, axis=1)\n",
    "\n",
    "for Input_File_Name, Pred_Class in zip(Input_Files, Prediction_Classes):\n",
    "    print('Prediction for {} is {}'.format(Input_File_Name, Labels_List[(Pred_Class.numpy()-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
